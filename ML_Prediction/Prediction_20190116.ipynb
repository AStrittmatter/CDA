{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PC Session 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Machine Learning for Prediction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Authors:**\n",
    "Jonathan Chassot, [Helge Liebert](https://hliebert.github.io/), and [Anthony Strittmatter](http://www.anthonystrittmatter.com)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You want to buy an used car at the online auction platform *MyLemons*. You are worried that many used cars on this platform are overpriced. Therefore, you want to do some market research before you buy an used car. For this purpose, you web-scrape the used car offers on the *MyLemons* platform. You obtain the following variables:\n",
    "\n",
    "\n",
    "|Variable name| Description|\n",
    "|:----|:----|\n",
    "|**Outcome variables** ||\n",
    "|*first_price*| First asking price in 1,000 CHF |\n",
    "|*final_price*| Transaction price in 1,000 CHF|\n",
    "|*overprice*| Dummy indicating *first_price > final_price* |\n",
    "|**Baseline covariates**| |\n",
    "|*bmw_320, opel_astra, mercedes_c, vw_golf, vw_passat*| Dummies for the car make and model|\n",
    "|*mileage*| Mileage of the used car (in 1,000 km)|\n",
    "|*age_car_years*| Age of the used car (in years)|\n",
    "|*diesel*| Dummy for diesel engines |\n",
    "|*private_seller*| Dummy for private seller (as opposed to professional used car sellers) |\n",
    "|*other_car_owner*| Number of previous caar owners |\n",
    "|*guarantee*| Dummy indicating that the seller offers a guarantee for the used car|\n",
    "|*maintenance_cert*| Dummy indicating that the seller has a complete maintenace certificate for the used car|\n",
    "|*inspection*| Categorial variable for the duration until next general inspection (3 categories: new, 1-2 years, > 2 years) |\n",
    "|*pm_green*| Dummy indicating that the used car has low particular matter emissions|\n",
    "|*co2_em*| CO2 emssion (in liter per 100km)|\n",
    "|*euro_norm*| EURO emission norm under which the car is registered |\n",
    "\n",
    "\n",
    "Furthermore, you generate some transformations of your covariates that you want to use for later analysis. The transformed covariates are:\n",
    "\n",
    "|Variable name| Description|\n",
    "|:----|:----|\n",
    "|**Additional covariates** ||\n",
    "|*mileage2, mileage3, mileage4, age_car_years2, age_car_years3, age_car_years4*| Squared, cubic, and quadratic *mileage* and *age_car_years* |\n",
    "|*mile_20, mile_30, mile_40, mile_50, mile_100, mile_150*| Dummies indicating that the used car has a mileage above 20,000km, 30,000km, 40,000km, 50,000km, 100,000km, or 150,000km |\n",
    "|*age_3, age_6*| Dummies indicating that the used car is above 3 or 6 years old |\n",
    "|*dur_next_ins_1_2*| Dummy indicating that the duration until the next general inspection is between 1 and 2 years |\n",
    "|*new_inspection*| Dummy indicating that the used car has a new general inspection |\n",
    "\n",
    "\n",
    "You store the prepared data in the file *used_cars.csv*. Now you start you analysis. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first place, you install and load the R-packages you want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################  Load Packages  ########################\n",
    "\n",
    "# List of required packages\n",
    "pkgs <- c('fBasics', 'corrplot', 'psych', 'glmnet', 'glmnetUtils', 'rpart',\n",
    "          'rpart.plot', 'treeClust', 'randomForest', 'rlang', 'readr', 'devtools',\n",
    "          'tidyverse', 'grf', 'reshape2', 'caret', 'neuralnet', 'plotmo', 'dmlmt')\n",
    "\n",
    "\n",
    "# Load packages\n",
    "for(pkg in pkgs){\n",
    "    library(pkg, character.only = TRUE)\n",
    "}\n",
    "\n",
    "print('All packages successfully installed and loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data Frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you load the data frame and define different covariate categories. You distinguish between binary and continuous/discrete covariates. Furthermore, you decrease the sample size to 10,000 in order to decrease the computation time while you are testing your code. You plan to use the entire sample of 78,645 used cars after you are finised with programming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################  Load Data Frame  ########################\n",
    "\n",
    "set.seed(100239) # set starting value for random number generator\n",
    "\n",
    "# Load data frame\n",
    "data_raw <- read.csv(\"used_cars.csv\",header=TRUE, sep=\",\")\n",
    "\n",
    "# Outcome Variables\n",
    "outcomes <- c(\"first_price\", \"final_price\", \"overprice\")\n",
    "\n",
    "# Covariates/Features\n",
    "baseline_covariates_bin <- c(\"bmw_320\", \"opel_astra\", \"mercedes_c\", \"vw_golf\", \"vw_passat\", \n",
    "                          \"diesel\",   \"private_seller\", \"guarantee\", \"maintenance_cert\",  \"pm_green\") # binary\n",
    "baseline_covariates_cont <- c(\"mileage\", \"age_car_years\", \"other_car_owner\", \"inspection\",\n",
    "                              \"co2_em\", \"euro_norm\") # continuous/ordered discrete\n",
    "baseline_covariates <- c(baseline_covariates_cont,baseline_covariates_bin)\n",
    "lasso_covariates_bin <- c(\"mile_20\", \"mile_30\", \"mile_40\", \"mile_50\", \"mile_100\", \"mile_150\", \n",
    "                       \"age_3\", \"age_6\",\"dur_next_ins_0\", \"dur_next_ins_1_2\", \"new_inspection\",\n",
    "                       \"euro_1\", \"euro_2\", \"euro_3\", \"euro_4\", \"euro_5\", \"euro_6\") # binary \n",
    "lasso_covariates_cont <- c(\"mileage2\", \"mileage3\", \"mileage4\", \"age_car_years2\", \"age_car_years3\",\n",
    "                           \"age_car_years4\") # continuous\n",
    "\n",
    "lasso_covariates <- c(lasso_covariates_cont, lasso_covariates_bin)  \n",
    "all_covariates <- c(baseline_covariates, lasso_covariates)\n",
    "all_variables <- c(outcomes, baseline_covariates, lasso_covariates)\n",
    "\n",
    "# Selection of Subsample size, max. 104,719 observations\n",
    "# Select smaller subsample to decrease computation time\n",
    "n_obs <- 10000\n",
    "df <- data_raw %>%\n",
    "  dplyr::sample_n(n_obs) %>%\n",
    "  dplyr::select(all_variables)\n",
    "\n",
    "print('Data frame successfully loaded and sample selected.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Means and Standard Deviations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have a look at the descriptive statistics to check for data errors and missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################  Table with Descriptive Statistics  ########################\n",
    "\n",
    "desc <- fBasics::basicStats(df) %>% t() %>% as.data.frame() %>% \n",
    "  select(Mean, Stdev, Minimum, Maximum, nobs)\n",
    "print(round(desc, digits=1))\n",
    "\n",
    "# Print as tex-file\n",
    "#kable(desc, \"latex\", booktabs = T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You plot the correlation matrix. The correlation structure between the covariates is important for the later interpretation of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################  Correlation Matrix  ########################\n",
    "\n",
    "corr = cor(df)\n",
    "corrplot(corr, type = \"upper\", tl.col = \"black\")\n",
    "\n",
    "# Save correlation matrix as png-file\n",
    "#png(height=1200, width=1200, file=\"correlation.png\")\n",
    "    #corrplot(corr, type = \"upper\", tl.col = \"black\")\n",
    "#dev.off()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You extract the different variable categories from your data frame and drop observations with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################  Extract Dataset  ########################\n",
    "\n",
    "# Extracting continuous variables\n",
    "baseline_covariates_cont <- df %>%\n",
    "  dplyr::select(baseline_covariates_cont) \n",
    "\n",
    "lasso_covariates_cont <- df %>%\n",
    "  dplyr::select(lasso_covariates_cont) \n",
    "\n",
    "# Extracting indicator variables\n",
    "baseline_covariates_bin <- df %>%\n",
    "  dplyr::select(baseline_covariates_bin)\n",
    "\n",
    "lasso_covariates_bin <- df %>%\n",
    "  dplyr::select(lasso_covariates_bin)\n",
    "\n",
    "# Extracting outcome \n",
    "outcomes <- df %>% dplyr::select(outcomes)\n",
    "\n",
    "# Setting up the data, renaming columns and discarding rows with NA (if any)\n",
    "df <- bind_cols(outcomes, baseline_covariates_cont, baseline_covariates_bin, lasso_covariates_cont, lasso_covariates_bin) %>%\n",
    "  na.omit()\n",
    "\n",
    "print('Data successfully extracted.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take Hold-Out-Sample and Scale Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You want to compare the relative prediction power of different estimation procedures based on the out-of-sample MSE and $R^2$. For this purpose, you create a training/estimation- and hold-out-sample. You will use the training/estimation-sample to train the machine learning procedures and estimate the final used car prices. Then you will extrapolate the results to the retared hold-out-sample.\n",
    "\n",
    "\n",
    "Furthermore, you standardise the continuous covariates by the means and standard deviations in the training/estimation-sample. Some machine learning algorithms are sensitive to rescaling. Even though most machine learning procedures standardise the covariates automatically, this is a cautionary transformation for those procedures that have no automatic standardisation implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################  Take Hold-Out-Sample  ########################\n",
    "\n",
    "df_part <- modelr::resample_partition(df, c(obs = 0.8, hold_out = 0.2))\n",
    "df_obs <- as.data.frame(df_part$obs) # Training and estimation sample\n",
    "df_hold_out <- as.data.frame(df_part$hold_out) # Hold-out-sample\n",
    "\n",
    "# Outcomes\n",
    "first_price_obs <- as.matrix(df_obs[,1])\n",
    "final_price_obs <- as.matrix(df_obs[,2])\n",
    "overprice_obs <- as.matrix(df_obs[,3])\n",
    "\n",
    "first_price_hold_out <- as.matrix(df_hold_out[,1])\n",
    "final_price_hold_out <- as.matrix(df_hold_out[,2])\n",
    "overprice_hold_out <- as.matrix(df_hold_out[,3])\n",
    "\n",
    "## Covariates/Features\n",
    "baseline_covariates_cont_obs <- as.matrix(df_obs[,c(4:9)])\n",
    "baseline_covariates_bin_obs <- as.matrix(df_obs[,c(10:19)])\n",
    "baseline_covariates_hold_cont_out <- as.matrix(df_hold_out[,c(4:9)])\n",
    "baseline_covariates_hold_bin_out <- as.matrix(df_hold_out[,c(10:19)])\n",
    "\n",
    "# Standardise continuous variables\n",
    "preProcValues <- preProcess(baseline_covariates_cont_obs, method = c(\"center\", \"scale\")) # Take means and standard deviations from training sample\n",
    "ObsTransformed <- predict(preProcValues, baseline_covariates_cont_obs) # Apply the transformation to trainings sample\n",
    "HoldOutTransformed <- predict(preProcValues, baseline_covariates_hold_cont_out) # Apply the transformation to hold-out-sample (based on means and standard deviations from training sample)\n",
    "# Note: Outcome variables are not rescaled\n",
    "\n",
    "baseline_covariates_obs <- as.matrix(cbind(ObsTransformed,baseline_covariates_bin_obs)) \n",
    "baseline_covariates_hold_out <- as.matrix(cbind(HoldOutTransformed,baseline_covariates_hold_bin_out)) \n",
    "                  \n",
    "print('The data is now ready for your first analysis!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You start the prediction of the final used car price with a tree estimator. Trees partition the sample into mutually exclusive groups $l_j$, which are called terminal leaves. Let $\\pi= \\{ l_1, ...,l_{\\# (\\pi)} \\}$ be a specific tree or sample partition, let $l_j \\equiv l_j (x,\\pi)$ be the respective terminal leaf (for $j=1,…,\\#(\\pi))$, and let $\\#(\\pi)$ be the number of terminal leaves in tree $\\pi$. The terminal leaf $l_j (x,\\pi)$ of tree $\\pi$ is a function of the covariates $X_{i}$ (for $i = 1,..., N$ used cars) such that $x \\in l_j$. \n",
    "\n",
    "For an explicit example, consider that $X_{i}$ contains only a binary indicator for used diesel cars. Then, you can choose between two possible trees; either you make no sample split, $\\pi'=\\{ l_1 \\}=\\{ \\mbox{diesel}, \\mbox{gasoline} \\}$, or you partition used diesel and gasoline cars into two separate leaves, $\\pi'' =\\{ l_1,l_2 \\}= \\{ \\{ \\mbox{diesel} \\}, \\{ \\mbox{gasoline} \\} \\}$.  \n",
    "\n",
    "Trees select the partition $\\pi$ by recursively adding leaves to the tree that minimise the MSE. Accordingly, trees aim to partition the sample into leaves with homogeneous outcomes. Because of the hierarchical partition structure, the first splits contribute more prediction power than the last splits. \n",
    "\n",
    "\n",
    "For a specific tree $\\pi$, you can predict the final used car price by\n",
    "\\begin{equation*}\n",
    "\\hat{E}[final\\_price|(x,\\pi)] = \\frac{1}{\\sum_{i=1}^{N}1\\{ X_{i} \\in l_j(x,\\pi)\\}}\\sum_{i=1}^{N} 1\\{ X_{i} \\in l_j(x,\\pi)\\} \\cdot final\\_price.\n",
    "\\end{equation*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shallow Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You start the analysis with a reletively small (shallow) tree, by imposing that each terminal leave needs to contain at least 500 used cars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################  Build Trees with different Leave Sizes  ########################                         \n",
    "\n",
    "# Prepare data for tree estimator\n",
    "tree_data_obs <-  data.frame(final_price_obs, baseline_covariates_obs)\n",
    "empty <- as.matrix(final_price_hold_out)\n",
    "empty[1,] <-NA\n",
    "tree_data_hold_out <- data.frame(rbind(final_price_obs,empty),rbind(baseline_covariates_obs, baseline_covariates_hold_out))\n",
    "\n",
    "# Setup the formula of the linear regression model\n",
    "sumx <- paste(baseline_covariates, collapse = \" + \")  \n",
    "linear <- paste(\"final_price_obs\",paste(sumx, sep=\" + \"), sep=\" ~ \")\n",
    "linear <- as.formula(linear)\n",
    "\n",
    "# Build the tree\n",
    "linear.singletree_1 <- rpart(formula = linear, data = tree_data_obs , method = \"anova\", xval = 10,\n",
    "                             y = TRUE, control = rpart.control(cp = 0.00002, minbucket=500))\n",
    "# Note: 'minbucket=500' imposes the restriction that each terminal leave should contain at least 500 used cars. Algorithm 'rpart' stops growing trees when either one leave has less than 500 observations or the MSE gain of addidng one addidtional leave is below cp=0.00002.\n",
    "\n",
    "print('Relative CV-MSE for different tree sizes')\n",
    "print(linear.singletree_1$cptable)\n",
    "\n",
    "# Plot CV-MSE\n",
    "plotcp(linear.singletree_1)\n",
    "\n",
    "# Save CV-MSE as png-file\n",
    "#png(filename= \"cp_tree1.png\", units=\"in\", width=5, height=4, pointsize=12, res=72)\n",
    "#plotcp(linear.singletree_1)\n",
    "#dev.off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, you select the tree that minimises the CV-MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################  Select the Tree that Minimises CV-MSE  ######################## \n",
    "\n",
    "op.index_1 <- which.min(linear.singletree_1$cptable[, \"xerror\"])\n",
    "print(paste0(\"Optimal number final leaves: \", op.index_1))\n",
    "\n",
    "# Get cp-value that corresponds to optimal tree size\n",
    "cp.vals_1 <- linear.singletree_1$cptable[op.index_1, \"CP\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CV-MSE is minimised at the maximum leave size of eight. This suggests that the tree is not deep enough to unfold the bias-variance trade-off. For illustrational purposes, we continue working with this shallow tree, but later we will relax the restrictions on the minimum leave size.\n",
    "\n",
    "Now you estimate the optimal tree and estimate out-of-sample RMSE and $R^2$ to compare the relative performance of different approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################  Select the Optimal Tree and Assess Out-of-Sample Performance  ######################## \n",
    "\n",
    "# Prune the tree\n",
    "treepruned.linearsingle_1 <- prune(linear.singletree_1, cp = cp.vals_1)\n",
    "\n",
    "# Predict final price in the observed and hold-out-samples\n",
    "pred_tree_hold_out_1 <- as.matrix(predict(treepruned.linearsingle_1, newdata=tree_data_hold_out))\n",
    "pred_tree_obs_1 <- pred_tree_hold_out_1[c(1:nrow(tree_data_obs)),]\n",
    "r <-nrow(final_price_obs)+1\n",
    "pred_tree_hold_out_1 <- pred_tree_hold_out_1[c(r:nrow(pred_tree_hold_out_1)),]\n",
    "\n",
    "## Assess performance of tree estimator\n",
    "# In-sample RMSE\n",
    "rmse_obs_1 <- round(sqrt(mean((final_price_obs - pred_tree_obs_1)^2)),digits=3)\n",
    "# Hold-out-sample RMSE\n",
    "rmse_hold_out_1 <- round(sqrt(mean((final_price_hold_out - pred_tree_hold_out_1)^2)),digits=3)\n",
    "# In-sample R-squared\n",
    "r2_obs_1 <- round(1-mean((final_price_obs - pred_tree_obs_1)^2)/mean((final_price_obs - mean(final_price_obs))^2),digits=3)\n",
    "# Hold-out-sample R-squared\n",
    "r2_hold_out_1 <- round(1-mean((final_price_hold_out - pred_tree_hold_out_1)^2)/mean((final_price_hold_out - mean(final_price_hold_out))^2),digits=3)\n",
    "\n",
    "print(paste0(\"In-Sample RMSE: \", rmse_obs_1))\n",
    "print(paste0(\"Hold-out-Sample RMSE: \", rmse_hold_out_1))\n",
    "print(paste0(\"In-Sample R-squared: \", r2_obs_1))\n",
    "print(paste0(\"Hold-out-Sample R-squared: \", r2_hold_out_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By splitting the sample into eight groups, you can explain already 54\\% of the out-of-sample *final_price*. Next, you visualise the tree by plotting the tree structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################  Visulatisation of tree  ######################## \n",
    "\n",
    "## Plot tree structure\n",
    "#plot(treepruned.linearsingle_1,digits=3)\n",
    "# Note: All continuous variables are standardised.\n",
    "\n",
    "# Save tree structure as png-file\n",
    "#png(filename= \"full_tree1.png\",units=\"in\", width=9, height=9, pointsize=12,res=72)\n",
    "rpart.plot(treepruned.linearsingle_1,digits=3)\n",
    "#dev.off()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have to be careful when interpreting the leaves of the tree. That some sample splits are based on, e.g., *car_age_years* does not necessarily mean that *car_age_years* causally affects *final_price*. The reason is, *car_age_years* is correlated with several other covartiates (e.g., *mileage* and *co2_em*). Possibly, the causal effect operates through one of the correlated covariates and *car_age_years* is just a good proxy. To demonstrate this, you plot in the next step the average regressor value for each leaf.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################  Plot Average Regresor Value for Each Leaf  ######################## \n",
    "## Code from Susan Athey and Guido Imbens AEA lecture\n",
    "\n",
    "# Take hold-out data only\n",
    "tree_data_out <- data.frame(pred_tree_hold_out_1, baseline_covariates_hold_out)\n",
    "\n",
    "# Map to each individual row the leaf number and add the covariates\n",
    "individual_leaf <- treeClust::rpart.predict.leaves(treepruned.linearsingle_1, tree_data_out)  %>% \n",
    "  as_tibble()  %>% \n",
    "  dplyr::rename(leaf=value) \n",
    "leaf_covariates <- cbind(individual_leaf, tree_data_out[baseline_covariates])\n",
    "\n",
    "# Get predicted final price of each leaf \n",
    "leaf_price <- treepruned.linearsingle_1$frame %>% as_tibble() %>%\n",
    "  dplyr::mutate(row = 1:nrow(.)) %>% \n",
    "  dplyr::filter(var == \"<leaf>\") %>% \n",
    "  dplyr::rename(leaf=row, pred_price=yval) %>% \n",
    "  dplyr::select(leaf, pred_price) \n",
    "\n",
    "# Merge all the information on leaf level\n",
    "leaf_data <- left_join(leaf_covariates, leaf_price, by=\"leaf\")\n",
    "\n",
    "# Mean of each covariate on each leaf, \n",
    "# Leafs sorted and renumbered by predicted prive\n",
    "leaf_mean <- leaf_data %>% \n",
    "  dplyr::group_by(leaf) %>%\n",
    "  dplyr::summarise_all(mean) %>%\n",
    "  dplyr::arrange(desc(pred_price)) %>%\n",
    "  dplyr::mutate(leaf = 1:nrow(.)) \n",
    "\n",
    "# Plot\n",
    "plt <- leaf_mean %>% \n",
    "  dplyr::select(leaf, baseline_covariates[c(16:1)]) %>%\n",
    "  melt(id=\"leaf\") %>%\n",
    "  ggplot(aes(x=factor(leaf), y=variable, fill=value)) +\n",
    "  geom_raster() +\n",
    "  scale_fill_gradient2() + \n",
    "  scale_x_discrete(breaks=seq_along(leaf_mean$pred_price),      \n",
    "                   labels=round(leaf_mean$pred_price, 1)) +\n",
    "  # From here on, all the code is optional styling\n",
    "  geom_tile(colour=\"white\",size=0.25) +            # white cell border\n",
    "  labs(x=\"Predicted final price\",\n",
    "       y=\"\", title=\"Average covariate value for each leaf\") +# axis labels \n",
    "  coord_fixed()+                                   # square cells\n",
    "  theme_grey(base_size=8)+                         # basic hue \n",
    "  theme(\n",
    "    axis.text=element_text(face=\"bold\"),      # axis font style\n",
    "    plot.background=element_blank(),          # cleaner background\n",
    "    panel.border=element_blank(),             # cleaner panel\n",
    "    legend.key.width=grid::unit(0.2,\"cm\"),    # slim legend color bar\n",
    "    axis.ticks=element_line(size=0.4),        # tick style\n",
    "    axis.text.x=element_text(size=7,          # tick label style\n",
    "                             colour=\"grey40\",\n",
    "                             angle = 60,\n",
    "                             hjust = 1),\n",
    "    plot.title=element_text(colour=\"grey40\",  # plot title style\n",
    "                            hjust=.5,size=7,\n",
    "                            face=\"bold\")\n",
    "  )\n",
    "\n",
    "plot(plt)\n",
    "\n",
    "# Save average covariate values as png-file\n",
    "#png(filename= \"regressor_values.png\",units=\"in\", width=6, height=6, pointsize=38,res=300)\n",
    "#plt\n",
    "#dev.off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure of the average covariates is similar for *milage*, *age_car_years* and negatively correlated with *inspection* and *euro_norm*. Accordingly, all of these variables could to some extend replace each other for the prediction of *final_price*. If anything, those covariates with little variation in the colour are less important to predict *final_price*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now you relax the restriction on the minimum leave size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## Deep tree estimator  ########################                         \n",
    "\n",
    "# Build deep Tree\n",
    "linear.singletree_2 <- rpart(formula = linear, data = tree_data_obs , method = \"anova\", xval = 10,\n",
    "                             y = TRUE, control = rpart.control(cp = 0.00002, minbucket=5))\n",
    "\n",
    "# Find tree size that minimises CV-MSE\n",
    "op.index_2 <- which.min(linear.singletree_2$cptable[, \"xerror\"])\n",
    "print(paste0(\"Optimal number final leaves: \", op.index_2))\n",
    "\n",
    "# Plot CV-MSE\n",
    "plotcp(linear.singletree_2)\n",
    "abline(v = op.index_2, lty = \"dashed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cp-value that corresponds to optimal tree size\n",
    "cp.vals_2 <- linear.singletree_2$cptable[op.index_2, \"CP\"]\n",
    "\n",
    "# Prune the tree\n",
    "treepruned.linearsingle_2 <- prune(linear.singletree_2, cp = cp.vals_2)\n",
    "\n",
    "# Plot tree structure\n",
    "rpart.plot(treepruned.linearsingle_2,digits=3)\n",
    "treepruned.linearsingle_2_short <- prune(linear.singletree_2, cp = 150*cp.vals_2)\n",
    "rpart.plot(treepruned.linearsingle_2_short,digits=3, main = \"First few leaves\",fallen.leaves=FALSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict final price in the observed and hold-out-samples\n",
    "pred_tree_hold_out_2 <- as.matrix(predict(treepruned.linearsingle_2, newdata=tree_data_hold_out))\n",
    "pred_tree_obs_2 <- pred_tree_hold_out_2[c(1:nrow(tree_data_obs)),]\n",
    "r <-nrow(final_price_obs)+1\n",
    "pred_tree_hold_out_2 <- pred_tree_hold_out_2[c(r:nrow(pred_tree_hold_out_2)),]\n",
    "\n",
    "## Assess performance of tree estimator\n",
    "# In-sample RMSE\n",
    "rmse_obs_2 <- round(sqrt(mean((final_price_obs - pred_tree_obs_2)^2)),digits=3)\n",
    "# Hold-out-sample RMSE\n",
    "rmse_hold_out_2 <- round(sqrt(mean((final_price_hold_out - pred_tree_hold_out_2)^2)),digits=3)\n",
    "# In-sample R-squared\n",
    "r2_obs_2 <- round(1-mean((final_price_obs - pred_tree_obs_2)^2)/mean((final_price_obs - mean(final_price_obs))^2),digits=3)\n",
    "# Hold-out-sample R-squared\n",
    "r2_hold_out_2 <- round(1-mean((final_price_hold_out - pred_tree_hold_out_2)^2)/mean((final_price_hold_out - mean(final_price_hold_out))^2),digits=3)\n",
    "\n",
    "print(paste0(\"In-Sample RMSE: \", rmse_obs_2))\n",
    "print(paste0(\"Hold-out-Sample RMSE: \", rmse_hold_out_2))\n",
    "print(paste0(\"In-Sample R-squared: \", r2_obs_2))\n",
    "print(paste0(\"Hold-out-Sample R-squared: \", r2_hold_out_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Honest Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## Honest deep tree estimator ########################                        \n",
    "\n",
    "# Create tratining and estimation sample\n",
    "df_obs_part <- modelr::resample_partition(df_obs, c(train = 0.5, est = 0.5))\n",
    "df_train <- as.data.frame(df_obs_part$train)\n",
    "df_est <- as.data.frame(df_obs_part$est)\n",
    "\n",
    "# Outcomes\n",
    "final_price_train <- as.matrix(df_train[,2])\n",
    "final_price_est <- as.matrix(df_est[,2])\n",
    "\n",
    "# Covariates/Features\n",
    "baseline_covariates_hold_cont_out <- as.matrix(df_hold_out[,c(4:9)])\n",
    "baseline_covariates_cont_train <- as.matrix(df_train[,c(4:9)]) \n",
    "baseline_covariates_cont_est <- as.matrix(df_est[,c(4:9)])\n",
    "baseline_covariates_hold_bin_out <- as.matrix(df_hold_out[,c(10:19)])\n",
    "baseline_covariates_bin_train <- as.matrix(df_train[,c(10:19)]) \n",
    "baseline_covariates_bin_est <- as.matrix(df_est[,c(10:19)])\n",
    "\n",
    "# Scale continuous variables\n",
    "preProcValues <- preProcess(baseline_covariates_cont_train, method = c(\"center\", \"scale\"))\n",
    "TrainTransformed <- predict(preProcValues, baseline_covariates_cont_train)\n",
    "HoldOutTransformed <- predict(preProcValues, baseline_covariates_hold_cont_out)\n",
    "EstTransformed <- predict(preProcValues, baseline_covariates_cont_est)\n",
    "\n",
    "baseline_covariates_hold_out <- as.matrix(cbind(HoldOutTransformed,baseline_covariates_hold_bin_out)) \n",
    "baseline_covariates_train <- as.matrix(cbind(TrainTransformed,baseline_covariates_bin_train)) \n",
    "baseline_covariates_est <- as.matrix(cbind(EstTransformed,baseline_covariates_bin_est)) \n",
    "\n",
    "# Prepare data for tree estimator\n",
    "tree_data_train <-  data.frame(final_price_train, baseline_covariates_train)\n",
    "empty1 <- as.matrix(final_price_train)\n",
    "empty1[1,] <-NA\n",
    "empty2 <- as.matrix(final_price_hold_out)\n",
    "empty2[1,] <-NA\n",
    "tree_data_hold_out <-  data.frame(rbind(empty1,final_price_est,empty2),rbind(baseline_covariates_train,baseline_covariates_est, baseline_covariates_hold_out))\n",
    "\n",
    "# Setup the formula of the linear regression model\n",
    "linear <- paste(\"final_price_train\",paste(sumx, sep=\" + \"), sep=\" ~ \")\n",
    "linear <- as.formula(linear)\n",
    "\n",
    "# Build deep tree\n",
    "linear.singletree_3 <- rpart(formula = linear, data = tree_data_train , method = \"anova\", xval = 10,\n",
    "                             y = TRUE, control = rpart.control(cp = 0.00002, minbucket=5))\n",
    "\n",
    "\n",
    "# Find tree size that minimises CV-MSE\n",
    "op.index_3 <- which.min(linear.singletree_3$cptable[, \"xerror\"])\n",
    "print(paste0(\"Optimal number final leaves: \", op.index_3))\n",
    "\n",
    "# Plot CV-MSE\n",
    "plotcp(linear.singletree_3)\n",
    "abline(v = op.index_3, lty = \"dashed\")\n",
    "\n",
    "# Get cp-value that corresponds to optimal tree size\n",
    "cp.vals_3 <- linear.singletree_3$cptable[op.index_3, \"CP\"]\n",
    "\n",
    "# Prune the tree\n",
    "treepruned.linearsingle_3 <- prune(linear.singletree_3, cp = cp.vals_3)\n",
    "\n",
    "# Predict final price in the observed and hold-out-samples\n",
    "pred_tree_hold_out_3 <- as.matrix(predict(treepruned.linearsingle_3, newdata=tree_data_hold_out))\n",
    "pred_tree_train_3 <- pred_tree_hold_out_3[c(1:nrow(final_price_train)),]\n",
    "r <-nrow(final_price_train)+1\n",
    "pred_tree_est_3 <- pred_tree_hold_out_3[c(r:nrow(final_price_obs)),]\n",
    "r <-nrow(final_price_obs)+1\n",
    "pred_tree_hold_out_3 <- pred_tree_hold_out_3[c(r:nrow(pred_tree_hold_out_3)),]\n",
    "\n",
    "## Assess performance of tree estimator\n",
    "# Training-sample RMSE\n",
    "rmse_train_3 <- round(sqrt(mean((final_price_train - pred_tree_train_3)^2)),digits=3)\n",
    "# Estimation-sample RMSE\n",
    "rmse_est_3 <- round(sqrt(mean((final_price_est - pred_tree_est_3)^2)),digits=3)\n",
    "# Hold-out-sample RMSE\n",
    "rmse_hold_out_3 <- round(sqrt(mean((final_price_hold_out - pred_tree_hold_out_3)^2)),digits=3)\n",
    "# Training-sample R-squared\n",
    "r2_train_3 <- round(1-mean((final_price_train - pred_tree_train_3)^2)/mean((final_price_train - mean(final_price_train))^2),digits=3)\n",
    "# Estimation-sample R-squared\n",
    "r2_est_3 <- round(1-mean((final_price_est - pred_tree_est_3)^2)/mean((final_price_est - mean(final_price_est))^2),digits=3)\n",
    "# Hold-out-sample R-squared\n",
    "r2_hold_out_3 <- round(1-mean((final_price_hold_out - pred_tree_hold_out_3)^2)/mean((final_price_hold_out - mean(final_price_hold_out))^2),digits=3)\n",
    "\n",
    "print(paste0(\"Training-Sample RMSE: \", rmse_train_3))\n",
    "print(paste0(\"Estimation-Sample RMSE: \", rmse_est_3))\n",
    "print(paste0(\"Hold-out-Sample RMSE: \", rmse_hold_out_3))\n",
    "print(paste0(\"Training-Sample R-squared: \", r2_train_3))\n",
    "print(paste0(\"Estimation-Sample R-squared: \", r2_est_3))\n",
    "print(paste0(\"Hold-out-Sample R-squared: \", r2_hold_out_3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Honest Cross-Fitted Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################  Crosfitted honest deep tree estimator  ########################                        \n",
    "\n",
    "# Alternate tratining and estimation sample\n",
    "tree_data_train1 <-  data.frame(final_price_train, baseline_covariates_train)\n",
    "tree_data_train2 <-  data.frame(final_price_est, baseline_covariates_est)\n",
    "\n",
    "# Prepare both data data for tree estimator\n",
    "empty1 <- as.matrix(final_price_train)\n",
    "empty1[1,] <-NA\n",
    "empty2 <- as.matrix(final_price_hold_out)\n",
    "empty2[1,] <-NA\n",
    "tree_data_hold_out1 <-  data.frame(rbind(empty1,final_price_est,empty2),rbind(baseline_covariates_train,baseline_covariates_est, baseline_covariates_hold_out))\n",
    "\n",
    "empty1 <- as.matrix(final_price_est)\n",
    "empty1[1,] <-NA\n",
    "empty2 <- as.matrix(final_price_hold_out)\n",
    "empty2[1,] <-NA\n",
    "tree_data_hold_out2 <-  data.frame(rbind(final_price_train,empty1,empty2),rbind(baseline_covariates_train,baseline_covariates_est, baseline_covariates_hold_out))\n",
    "\n",
    "#####\n",
    "\n",
    "# Setup the formula of the linear regression model for the first tree\n",
    "linear <- paste(\"final_price_train\",paste(sumx, sep=\" + \"), sep=\" ~ \")\n",
    "linear <- as.formula(linear)\n",
    "\n",
    "# Build first deep tree\n",
    "linear.singletree_4 <- rpart(formula = linear, data = tree_data_train1 , method = \"anova\", xval = 10,\n",
    "                             y = TRUE, control = rpart.control(cp = 0.00002, minbucket=5))\n",
    "\n",
    "\n",
    "# Find tree size that minimises CV-MSE\n",
    "op.index_4 <- which.min(linear.singletree_4$cptable[, \"xerror\"])\n",
    "print(paste0(\"Optimal number final leaves: \", op.index_4))\n",
    "\n",
    "# Plot CV-MSE\n",
    "plotcp(linear.singletree_4)\n",
    "abline(v = op.index_4, lty = \"dashed\")\n",
    "\n",
    "# Get cp-value that corresponds to optimal tree size\n",
    "cp.vals_4 <- linear.singletree_4$cptable[op.index_4, \"CP\"]\n",
    "\n",
    "# Prune the tree\n",
    "treepruned.linearsingle_4 <- prune(linear.singletree_4, cp = cp.vals_4)\n",
    "\n",
    "# Predict final price in the hold-out-sample\n",
    "pred_tree_hold_out_4 <- as.matrix(predict(treepruned.linearsingle_4, newdata=tree_data_hold_out1))\n",
    "r <-nrow(final_price_obs)+1\n",
    "pred_tree_hold_out_4 <- pred_tree_hold_out_4[c(r:nrow(pred_tree_hold_out_4)),]\n",
    "\n",
    "#####\n",
    "\n",
    "# Setup the formula of the linear regression model for the second tree\n",
    "linear <- paste(\"final_price_est\",paste(sumx, sep=\" + \"), sep=\" ~ \")\n",
    "linear <- as.formula(linear)\n",
    "\n",
    "# Build second deep tree\n",
    "linear.singletree_5 <- rpart(formula = linear, data = tree_data_train2 , method = \"anova\", xval = 10,\n",
    "                             y = TRUE, control = rpart.control(cp = 0.00000001, minbucket=5))\n",
    "\n",
    "\n",
    "# Find tree size that minimises CV-MSE\n",
    "op.index_5 <- which.min(linear.singletree_5$cptable[, \"xerror\"])\n",
    "print(paste0(\"Optimal number final leaves: \", op.index_5))\n",
    "\n",
    "# Plot CV-MSE\n",
    "plotcp(linear.singletree_5)\n",
    "abline(v = op.index_5, lty = \"dashed\")\n",
    "\n",
    "# Get cp-value that corresponds to optimal tree size\n",
    "cp.vals_5 <- linear.singletree_5$cptable[op.index_5, \"CP\"]\n",
    "\n",
    "# Prune the tree\n",
    "treepruned.linearsingle_5 <- prune(linear.singletree_5, cp = cp.vals_5)\n",
    "\n",
    "# Predict final price in the hold-out-sample\n",
    "pred_tree_hold_out_5 <- as.matrix(predict(treepruned.linearsingle_5, newdata=tree_data_hold_out2))\n",
    "r <-nrow(final_price_obs)+1\n",
    "pred_tree_hold_out_5 <- pred_tree_hold_out_5[c(r:nrow(pred_tree_hold_out_5)),]\n",
    "\n",
    "#####\n",
    "\n",
    "## Assess performance of tree estimator\n",
    "# Hold-out-sample RMSE\n",
    "rmse_hold_out_4 <- round(sqrt(mean((final_price_hold_out - 0.5*(pred_tree_hold_out_4 + pred_tree_hold_out_5))^2)),digits=3)\n",
    "# Hold-out-sample R-squared\n",
    "r2_hold_out_4 <- round(1-mean((final_price_hold_out - 0.5*(pred_tree_hold_out_4 + pred_tree_hold_out_5))^2)/mean((final_price_hold_out - mean(final_price_hold_out))^2),digits=3)\n",
    "\n",
    "print(paste0(\"Hold-out-Sample RMSE: \", rmse_hold_out_4))\n",
    "print(paste0(\"Hold-out-Sample R-squared: \", r2_hold_out_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################  Random forest estimator  ######################## \n",
    "\n",
    "# Tuning parameters\n",
    "min_tree = 1\n",
    "num_trees = 500\n",
    "cov_frac = 2/3\n",
    "\n",
    "# Build random forest\n",
    "forest <- regression_forest(baseline_covariates_obs, final_price_obs, sample.fraction = 0.5, \n",
    "                  mtry = floor(cov_frac*ncol(baseline_covariates_obs)),\n",
    "                  num.trees = num_trees, min.node.size = min_tree,\n",
    "                  honesty = TRUE, honesty.fraction = 0.5)\n",
    "\n",
    "# Predict prices in hold-out-sample\n",
    "pred_forest <- predict(forest, newdata = baseline_covariates_hold_out)\n",
    "\n",
    "## Assess performance of forest estimator\n",
    "# Hold-out-sample RMSE\n",
    "rmse_forest <- round(sqrt(mean((final_price_hold_out - pred_forest$predictions)^2)),digits=3)\n",
    "# Hold-out-sample R-squared\n",
    "r2_forest <- round(1-mean((final_price_hold_out - pred_forest$predictions)^2)/mean((final_price_hold_out - mean(final_price_hold_out))^2),digits=3)\n",
    "\n",
    "print(paste0(\"Hold-out-Sample RMSE: \", rmse_forest))\n",
    "print(paste0(\"Hold-out-Sample R-squared: \", r2_forest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of splits by covariate and tree depht\n",
    "split <- split_frequencies(forest, max.depth = 4)\n",
    "colnames(split) <- baseline_covariates\n",
    "print(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select tuning parameters for forest\n",
    "#for_sizes = c(1,5, 10,15, 20,25,50,100,150,200,250, 300,350, 400, 450, 500, 1000, 1500, 2000, 3000, 4000, 5000, 10000)\n",
    "for_sizes = c(1,5, 10,15, 20,25,50,100,150,200,250, 300,350, 400, 450, 500)\n",
    "auc <- matrix(NA,nrow=length(for_sizes),ncol=3)\n",
    "ctr <- 0\n",
    "for (n in for_sizes){\n",
    "  ctr <- ctr + 1\n",
    "  auc[ctr,1] <- n\n",
    "  \n",
    "  forest <- regression_forest(baseline_covariates_obs, final_price_obs, sample.fraction = 0.5, \n",
    "                              mtry = floor(cov_frac*ncol(baseline_covariates_obs)),\n",
    "                              num.trees = n, min.node.size = min_tree,\n",
    "                              honesty = TRUE, honesty.fraction = 0.5)\n",
    "  \n",
    "  # Predict prices in hold-out-sample\n",
    "  pred_forest <- predict(forest, newdata = baseline_covariates_hold_out)\n",
    "  rmse_forest <- round(sqrt(mean((final_price_hold_out - pred_forest$predictions)^2)),digits=3)\n",
    "  auc[ctr,2] <- rmse_forest\n",
    "  if (ctr >1) {\n",
    "    auc[ctr,3] <- rmse_forest-auc[ctr-1,2]\n",
    "  }\n",
    "}\n",
    "\n",
    "plot(auc[,1], auc[,2], main=\"Tuning of forest size\", \n",
    "     xlab=\"Number of trees in forest \", ylab=\"RMSE\", pch=19)\n",
    "# fit a non-linear regression\n",
    "nls_fit <- lm(auc[,2] ~  auc[,1] + I(auc[,1]^(1/2)) + I(auc[,1]^2) + I(auc[,1]^3) + I(log(auc[,1])))\n",
    "lines(auc[,1], predict(nls_fit), col = \"red\")\n",
    "\n",
    "plot(auc[c(2:nrow(auc)),1], auc[c(2:nrow(auc)),3], main=\"Tuning of forest size\", \n",
    "     xlab=\"Number of trees in forest \", ylab=\"Delta RMSE\", pch=19)\n",
    "# fit a non-linear regression\n",
    "nls_fit <- lm(auc[c(2:nrow(auc)),3] ~  auc[c(2:nrow(auc)),1] + I(auc[c(2:nrow(auc)),1]^(1/2)) + I(auc[c(2:nrow(auc)),1]^2) + I(auc[c(2:nrow(auc)),1]^3) + I(log(auc[c(2:nrow(auc)),1])))\n",
    "lines(auc[c(2:nrow(auc)),1], predict(nls_fit), col = \"red\")\n",
    "abline(h=0)\n",
    "\n",
    "\n",
    "# Save graph\n",
    "#png(filename= \"auc.png\",units=\"in\", width=6, height=6, pointsize=10,res=300)\n",
    "#  plot(auc[,1], auc[,2], main=\"Tuning of forest size\", \n",
    "#       xlab=\"Number of trees in forest \", ylab=\"RMSE\", pch=19)\n",
    "#  # fit a non-linear regression\n",
    "#  nls_fit <- lm(auc[,2] ~  auc[,1] + I(auc[,1]^(1/2)) + I(auc[,1]^2) + I(auc[,1]^3) + I(log(auc[,1])))\n",
    "#  lines(auc[,1], predict(nls_fit), col = \"red\")\n",
    "#dev.off()\n",
    "\n",
    "#png(filename= \"delta_auc.png\",units=\"in\", width=6, height=6, pointsize=10,res=300)\n",
    "#  plot(auc[c(2:nrow(auc)),1], auc[c(2:nrow(auc)),3], main=\"Tuning of forest size\", \n",
    "#       xlab=\"Number of trees in forest \", ylab=\"Delta RMSE\", pch=19)\n",
    "#  # fit a non-linear regression\n",
    "#  nls_fit <- lm(auc[c(2:nrow(auc)),3] ~  auc[c(2:nrow(auc)),1] + I(auc[c(2:nrow(auc)),1]^(1/2)) + I(auc[c(2:nrow(auc)),1]^2) + I(auc[c(2:nrow(auc)),1]^3) + I(log(auc[c(2:nrow(auc)),1])))\n",
    "#  lines(auc[c(2:nrow(auc)),1], predict(nls_fit), col = \"red\")\n",
    "#  abline(h=0)\n",
    "#dev.off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LASSO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare Lasso data###\n",
    "\n",
    "# Generate some noisy covariates to disturbe the estimation\n",
    "noise.covars.hold.out <- matrix(data = rnorm(nrow(df_hold_out)),  nrow = nrow(df_hold_out), ncol = 13)\n",
    "colnames(noise.covars.hold.out) <- c(\"noise1\", \"noise2\", \"noise3\", \"noise4\", \"noise5\", \"noise6\", \"noise7\", \"noise8\", \"noise9\", \"noise10\", \"noise11\", \"noise12\",\"noise13\")\n",
    "\n",
    "noise.covars.est <- matrix(data = rnorm(nrow(df_est)),  nrow = nrow(df_est), ncol = 13)\n",
    "colnames(noise.covars.est) <- c(\"noise1\", \"noise2\", \"noise3\", \"noise4\", \"noise5\", \"noise6\", \"noise7\", \"noise8\", \"noise9\", \"noise10\", \"noise11\", \"noise12\",\"noise13\")\n",
    "\n",
    "noise.covars.train <- matrix(data = rnorm(nrow(df_train)),  nrow = nrow(df_train), ncol = 13)\n",
    "colnames(noise.covars.train) <- c(\"noise1\", \"noise2\", \"noise3\", \"noise4\", \"noise5\", \"noise6\", \"noise7\", \"noise8\", \"noise9\", \"noise10\", \"noise11\", \"noise12\",\"noise13\")\n",
    "\n",
    "ncol(df_hold_out)\n",
    "# Prepare LASSO variables\n",
    "lasso_covariates_hold_out_cont <- as.matrix(df_hold_out[,c(4:6,8,20:25)])\n",
    "lasso_covariates_est_cont <- as.matrix(df_est[,c(4:6,8,20:25)])\n",
    "lasso_covariates_train_cont <- as.matrix(df_train[,c(4:6,8,20:25)])\n",
    "\n",
    "# Scale continuous variables\n",
    "preProcValuesLasso <- preProcess(lasso_covariates_train_cont, method = c(\"center\", \"scale\"))\n",
    "TrainTransformed <- predict(preProcValuesLasso, lasso_covariates_train_cont)\n",
    "HoldOutTransformed <- predict(preProcValuesLasso, lasso_covariates_hold_out_cont)\n",
    "EstTransformed <- predict(preProcValuesLasso, lasso_covariates_est_cont)\n",
    "\n",
    "lasso_covariates_hold_out <- as.matrix(cbind(HoldOutTransformed,df_hold_out[,c(10:19, 26:ncol(df_hold_out))],noise.covars.hold.out)) \n",
    "lasso_covariates_train <- as.matrix(cbind(TrainTransformed,df_train[,c(10:19,26:ncol(df_train))],noise.covars.train)) \n",
    "lasso_covariates_est <- as.matrix(cbind(EstTransformed,df_est[,c(10:19,26:ncol(df_est))],noise.covars.est)) \n",
    "\n",
    "#########################\n",
    "print('Lasso data ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lasso.linear <- cv.glmnet(lasso_covariates_train, final_price_train,alpha=1,type.measure = 'mse', parallel=FALSE)\n",
    "plot(lasso.linear)\n",
    "print(lasso.linear)\n",
    "\n",
    "#ncol(lasso_covariates_train)\n",
    "#lm.fit(lasso_covariates_train, final_price_train)\n",
    "# 36 degrees of freedom\n",
    "\n",
    "\n",
    "print(paste0(\"Lambda minimising CV-MSE: \", round(lasso.linear$lambda.min,digits=8)))\n",
    "# 1 standard error rule reduces the number of included covariates\n",
    "print(paste0(\"Lambda 1 standard error rule: \", round(lasso.linear$lambda.1se,digits=8)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Lasso Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "######################################\n",
    "\n",
    "## Plot Lasso structure\n",
    "\n",
    "lambda_min = lasso.linear$glmnet.fit$lambda[50]\n",
    "\n",
    "\n",
    "mod <- glmnet(lasso_covariates_train , final_price_train,lambda.min = lambda_min,  alpha=1)\n",
    "glmcoef<-coef(mod,lasso.linear$lambda.1se)\n",
    "coef.increase<-dimnames(glmcoef[glmcoef[,1]>0,0])[[1]]\n",
    "coef.decrease<-dimnames(glmcoef[glmcoef[,1]<0,0])[[1]]\n",
    "\n",
    "maxcoef<-coef(mod,s=lambda_min)\n",
    "coef<-dimnames(maxcoef[maxcoef[,1]!=0,0])[[1]]\n",
    "allnames<-dimnames(maxcoef[maxcoef[,1]!=0,0])[[1]][order(maxcoef[maxcoef[,1]!=0,ncol(maxcoef)],decreasing=TRUE)]\n",
    "allnames<-setdiff(allnames,allnames[grep(\"Intercept\",allnames)])\n",
    "\n",
    "\n",
    "\n",
    "#assign colors\n",
    "cols<-rep(\"gray\",length(allnames))\n",
    "cols[allnames %in% coef.increase]<-\"red\"      # higher mpg is good\n",
    "cols[allnames %in% coef.decrease]<- \"green\"        # lower mpg is not\n",
    "\n",
    "plot_glmnet(mod,label=TRUE,s=lasso.linear$lambda.1se,col= cols)\n",
    "\n",
    "#Plot LASSO Coefficients\n",
    "print(glmcoef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Estimate LASSO model with 1 standard error lambda in estimation sample\n",
    "lasso.fit <- glmnet(lasso_covariates_est , final_price_est,lambda = lasso.linear$lambda.1se)\n",
    "\n",
    "# Extrapolate LASSO estimates to hold-out-sample\n",
    "yhat.lasso <- predict(lasso.fit, lasso_covariates_hold_out)\n",
    "\n",
    "## Assess performance of LASSO estimator\n",
    "# Hold-out-sample RMSE\n",
    "rmse_lasso <- round(sqrt(mean((final_price_hold_out - yhat.lasso)^2)),digits=3)\n",
    "# Hold-out-sample R-squared\n",
    "r2_lasso <- round(1-mean((final_price_hold_out - yhat.lasso)^2)/mean((final_price_hold_out - mean(final_price_hold_out))^2),digits=3)\n",
    "\n",
    "print(paste0(\"Hold-out-Sample RMSE: \", rmse_lasso))\n",
    "print(paste0(\"Hold-out-Sample R-squared: \", r2_lasso))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################  Post-LASSO estimator  ########################\n",
    "\n",
    "# Estimate LASSO model with 1 standard error lambda in training sample\n",
    "lasso.linear.fit <- glmnet(lasso_covariates_train , final_price_train,lambda = lasso.linear$lambda.1se)\n",
    "\n",
    "# Select covariates with non-zero coefficients\n",
    "coef <- predict(lasso.linear.fit, type = \"nonzero\") # Method 2\n",
    "colnames <- colnames(lasso_covariates_train)\n",
    "selected.vars <- colnames[unlist(coef)]\n",
    "\n",
    "# Prepare (unscaled) LASSO variables\n",
    "lasso_covariates_hold_out <- as.data.frame(cbind(df_hold_out[,c(4:ncol(df_hold_out))],noise.covars.hold.out)) \n",
    "lasso_covariates_est <- as.data.frame(cbind(df_est[,c(4:ncol(df_est))],noise.covars.est)) \n",
    "\n",
    "\n",
    "# Linear Post-LASSO fit in estimation sample\n",
    "post.lasso.model <- paste(\"final_price_est\", paste(selected.vars,collapse=\" + \"),  sep = \" ~ \") \n",
    "post.lasso.model <- as.formula(post.lasso.model)\n",
    "post.lasso <- lm(post.lasso.model, data=lasso_covariates_est)\n",
    "summary(post.lasso)\n",
    "\n",
    "# Extrapolate Post-LASSO estimates to hold-out-sample\n",
    "yhat.post.lasso <- predict(post.lasso, newdata=lasso_covariates_hold_out)\n",
    "\n",
    "## Assess performance of LASSO estimator\n",
    "# Hold-out-sample RMSE\n",
    "rmse_post_lasso <- round(sqrt(mean((final_price_hold_out - yhat.post.lasso)^2)),digits=3)\n",
    "# Hold-out-sample R-squared\n",
    "r2_post_lasso <- round(1-mean((final_price_hold_out - yhat.post.lasso)^2)/mean((final_price_hold_out - mean(final_price_hold_out))^2),digits=3)\n",
    "\n",
    "print(paste0(\"Hold-out-Sample RMSE: \", rmse_post_lasso))\n",
    "print(paste0(\"Hold-out-Sample R-squared: \", r2_post_lasso))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Lasso CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "########################  Post-LASSO estimator with correct CV  ########################\n",
    "# By MCKnaus\n",
    "\n",
    "\n",
    "lasso.linear <- post_lasso_cv(lasso_covariates_train, final_price_train,lambda.min = exp(-9), \n",
    "                              alpha=1, parallel=FALSE, output = T, se_rule = c(-1))\n",
    "lasso.linear$names_Xse_pl$`SE -1`[-1]\n",
    "\n",
    "\n",
    "\n",
    "# Extrapolate Post-LASSO estimates to hold-out-sample\n",
    "post.lasso.model <- paste(\"final_price_est\", paste(lasso.linear$names_Xse_pl$`SE -1`[-1],collapse=\" + \"),  sep = \" ~ \") \n",
    "post.lasso.model <- as.formula(post.lasso.model)\n",
    "post.lasso <- lm(post.lasso.model, data=lasso_covariates_est)\n",
    "summary(post.lasso)\n",
    "\n",
    "\n",
    "yhat.post.lasso <- predict(post.lasso, newdata=lasso_covariates_hold_out)\n",
    "\n",
    "## Assess performance of LASSO estimator\n",
    "# Hold-out-sample RMSE\n",
    "rmse_post_lasso <- round(sqrt(mean((final_price_hold_out - yhat.post.lasso)^2)),digits=3)\n",
    "# Hold-out-sample R-squared\n",
    "r2_post_lasso <- round(1-mean((final_price_hold_out - yhat.post.lasso)^2)/mean((final_price_hold_out - mean(final_price_hold_out))^2),digits=3)\n",
    "\n",
    "print(paste0(\"Hold-out-Sample RMSE: \", rmse_post_lasso))\n",
    "print(paste0(\"Hold-out-Sample R-squared: \", r2_post_lasso))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logit Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################  Logit-LASSO   ########################\n",
    "\n",
    "overprice_train <- as.matrix(df_train[,3])\n",
    "overprice_est <- as.matrix(df_est[,3])\n",
    "lasso_covariates_est <- as.matrix(lasso_covariates_est)\n",
    "lasso_covariates_hold_out <- as.matrix(lasso_covariates_hold_out)\n",
    "\n",
    "# Cross-validate Lambda of LASSO model##\n",
    "\n",
    "lasso.logit <- cv.glmnet(lasso_covariates_train, overprice_train,alpha=1,family='binomial',type.measure = 'mse', parallel=FALSE)\n",
    "plot(lasso.logit)\n",
    "\n",
    "print(paste0(\"Lambda minimising CV-MSE: \", round(lasso.logit$lambda.min,digits=8)))\n",
    "# 1 standard error rule reduces the number of included covariates\n",
    "print(paste0(\"Lambda 1 standard error rule: \", round(lasso.logit$lambda.1se,digits=8)))\n",
    "\n",
    "\n",
    "\n",
    "# Estimate LASSO model with 1 standard error lambda in estimation sample\n",
    "lasso.logit.fit <- glmnet(lasso_covariates_est , overprice_est, alpha=1, family='binomial', lambda = lasso.logit$lambda.1se)\n",
    "mean(overprice_est)\n",
    "coef(lasso.logit.fit, s= lasso.logit$lambda.1se)\n",
    "\n",
    "# Extrapolate LASSO estimates to hold-out-sample\n",
    "yhat.lasso.logit <- predict(lasso.logit.fit, lasso_covariates_hold_out, lambda = lasso.logit$lambda.1se, type='response')\n",
    "\n",
    "## Assess performance of LASSO estimator\n",
    "# Hold-out-sample RMSE\n",
    "rmse_lasso_logit <- round(sqrt(mean((overprice_hold_out - yhat.lasso.logit)^2)),digits=3)\n",
    "# Hold-out-sample R-squared\n",
    "r2_lasso_logit <- round(1-mean((overprice_hold_out - yhat.lasso.logit)^2)/mean((overprice_hold_out - mean(overprice_hold_out))^2),digits=3)\n",
    "\n",
    "print(paste0(\"Hold-out-Sample RMSE: \", rmse_lasso_logit))\n",
    "print(paste0(\"Hold-out-Sample R-squared: \", r2_lasso_logit))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Car Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################  Load New Car data  ########################\n",
    "# 26251 new cars on the market\n",
    "\n",
    "# Load data frame\n",
    "data_new <- read.csv(\"used_cars_on_market.csv\",header=TRUE, sep=\",\")\n",
    "df_new <- data_new %>%\n",
    "  dplyr::select(c(\"id\",all_covariates))\n",
    "\n",
    "# Covariates/Features\n",
    "baseline_covariates_bin <- c(\"bmw_320\", \"opel_astra\", \"mercedes_c\", \"vw_golf\", \"vw_passat\", \n",
    "                             \"diesel\",   \"private_seller\", \"guarantee\", \"maintenance_cert\",  \"pm_green\") # binary\n",
    "baseline_covariates_cont <- c(\"mileage\", \"age_car_years\", \"other_car_owner\", \"inspection\",\n",
    "                              \"co2_em\", \"euro_norm\") # continuous/ordered discrete\n",
    "baseline_covariates <- c(baseline_covariates_cont,baseline_covariates_bin)\n",
    "lasso_covariates_bin <- c(\"mile_20\", \"mile_30\", \"mile_40\", \"mile_50\", \"mile_100\", \"mile_150\", \n",
    "                          \"age_3\", \"age_6\",\"dur_next_ins_0\", \"dur_next_ins_1_2\", \"new_inspection\",\n",
    "                          \"euro_1\", \"euro_2\", \"euro_3\", \"euro_4\", \"euro_5\", \"euro_6\") # binary \n",
    "lasso_covariates_cont <- c(\"mileage2\", \"mileage3\", \"mileage4\", \"age_car_years2\", \"age_car_years3\",\n",
    "                           \"age_car_years4\") # continuous\n",
    "\n",
    "\n",
    "# Extracting continuous variables\n",
    "baseline_covariates_cont_new <- df_new %>%\n",
    "  dplyr::select(baseline_covariates_cont) \n",
    "\n",
    "lasso_covariates_cont_new <- df_new %>%\n",
    "  dplyr::select(lasso_covariates_cont) \n",
    "\n",
    "# Extracting indicator variables\n",
    "baseline_covariates_bin_new <- df_new %>%\n",
    "  dplyr::select(baseline_covariates_bin)\n",
    "\n",
    "lasso_covariates_bin_new <- df_new %>%\n",
    "  dplyr::select(lasso_covariates_bin)\n",
    "\n",
    "# Extracting outcome \n",
    "id <- data_new %>% dplyr::select(\"id\")\n",
    "\n",
    "# Setting up the data, renaming columns and discarding rows with NA (if any)\n",
    "df_new <- bind_cols(id, baseline_covariates_cont_new, baseline_covariates_bin_new, lasso_covariates_cont_new, lasso_covariates_bin_new) %>%\n",
    "  na.omit()\n",
    "\n",
    "\n",
    "# Baseline Covariates\n",
    "baseline_covariates_cont_train_old <- as.matrix(df_train[,c(4:9)]) \n",
    "preProcValues <- preProcess(baseline_covariates_cont_train_old , method = c(\"center\", \"scale\")) # Take means and standard deviations from training sample\n",
    "NewTransformed <- predict(preProcValues, baseline_covariates_cont_new)\n",
    "baseline_covariates_new <- as.matrix(cbind(NewTransformed,baseline_covariates_bin_new)) \n",
    "\n",
    "\n",
    "# Lasso Covariates\n",
    "l_cov_train_cont_old <- as.matrix(df_train[,c(4:6,8,20:25)])\n",
    "l_cov_train_cont_new <- as.matrix( bind_cols(baseline_covariates_cont_new,lasso_covariates_cont_new))\n",
    "l_cov_train_bin_new <- as.matrix( bind_cols(baseline_covariates_bin_new,lasso_covariates_bin_new))\n",
    "noise.covars.new <- matrix(data = rnorm(nrow(df_new)),  nrow = nrow(df_new), ncol = 13)\n",
    "colnames(noise.covars.new) <- c(\"noise1\", \"noise2\", \"noise3\", \"noise4\", \"noise5\", \"noise6\", \"noise7\", \"noise8\", \"noise9\", \"noise10\", \"noise11\", \"noise12\",\"noise13\")\n",
    "preProcValues <- preProcess(l_cov_train_cont_old, method = c(\"center\", \"scale\"))\n",
    "NewTransL <- predict(preProcValues, l_cov_train_cont_new)\n",
    "lasso_covariates_new <- as.matrix(cbind(NewTransL,l_cov_train_bin_new,noise.covars.new)) \n",
    "\n",
    "\n",
    "print('New data successfully extracted.')\n",
    "nrow(df_new)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save predictions\n",
    "predictions = matrix(NA,ncol=2, nrow=nrow(id))\n",
    "predictions <- cbind(df_new[,1],predictions)\n",
    "colnames(predictions) <- c(\"id\",\"final_price\", \"overprice\")\n",
    "write.csv(predictions, file = \"predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
